\section{Design of MURS}
\label{sec:desgin}

Memory pressure essentially describes the usage of heap in managed languages. In MURS, we first compute the heavy tasks which may have the linear or super-linear models, or have a large input dataset. The fundamental scheduling idea of MURS is to suspend the heavy tasks as the memory pressure occurs, and resume the suspended tasks when the memory pressure recedes or light tasks are completed.

%\subsection{Memory management}

%// 这一段的目的不够明确

%Although the memory usage rate describes the memory usage features about different tasks, we should also separate the memory management of the data processing system itself and JVM. Because the data processing systems always take some measures to avoid the out of memory error by spilling data to disk which based on the memory management themselves. When the spill occurs, the memory may suffer pressures although we stop tasks.

%Most data processing systems split the allocated memory to cache memory and execution memory. The cache memory is only used to store data with long lifetime. While execution memory is only used for temporary or middle data, such as shuffle result that will be write to disk. Cache memory and execution memory can be managed uniformly. What's more, these execution memory are allocated to each task independently. These make up the memory management of data processing system and also decide the memory usage rate of one task based the function API. When considering about the memory pressure, it's conflict to consider the memory management of data processing system. Because after one task is completed, the allocated execution memory will be reclaimed but the data objects are also in JVM heap. On another hand, some tasks will not only use the execution memory but also cache memory. Thus, in order to measure the memory pressure, we just consider the usage of JVM heap but not the memory management of data processing system.

%In the memory management of JVM, the heap space is split to young generation, old generation and other generations. The pinch of young generation leads to \textit{Minor GC}. Minor GC will move the lived data objects from young generation to old generation. The pinch of old generation leads to Full GC. If most long lifetime data objects remain in the old generation, frequent full gc will occur and lead to bad performance. Some garbage collection algorithms are working only in young generation or old generation, and some can works on both generations. In any case, the garbage collection is triggered when the used space of heap get the threshold.

Firstly, the memory pressure occurs when the proportion of the used heap has reached the threshold value. The threshold is set based on the triggering condition of garbage collection. In addition, we set two thresholds in the scheduler of MURS. The first threshold, called the yellow value, is used to indicate the memory pressure. When the percentage of the long living data objects in the heap meets the yellow value, it suggests the frequent full GC will occur. Full GC means that the garbage collector will clean all the heap, which is usually computationally expensive. Another threshold value, called as red value, is used to avoid spilling. The red value represents the level of memory pressure under which the out-of-memory error will occur or suggests that some data will be spilled to the disk. The default yellow and red values are 0.4 and 0.8, respectively, based on our evaluation. When the memory pressure is high, we will reduce the value.

With the yellow and red values, an accurate percentage of long living data objects in the heap actually determines the efficiency of the threshold. JVM splits the heap to young generation and old generation. Minor GC cleans the young generation and moves living data objects to the old generation. Thus the percentage of the heap usage after a minor GC represents the living data objects in the heap. After each full GC, dead data objects in old generation will also be reclaimed and then we revise the percentage of long living data objects in the heap according to the current percentage of heap usage.

%With the yellow value and red value, the percentage of data objects with long lifetime in the heap are actually important. As we know, garbage collection algorithm is based on mark-clean; thus, long-lived data objects in the heap will lead to expensive marking and cleaning costs. What’s more, long-lived data objects directly result in frequent full garbage collection. In order to get the percent of long lifetime data objects in heap, we set it to the size after a minor GC, which just cleans parts of the heap quickly. These data objects are lived in the heap, and most will also be lived along with the tasks.  

%Although the scheduler wishes to reclaim as many data objects as possible, the space reclaimed the last time will have an error with the expected reclaimed space. The error actually means the space occupied by these data objects was stored in an old generation and reclaimed before the task completed. We count the error and consider it in the last computation. Because tasks themselves work with the iterator, the produced data objects have steady regulars.

\IncMargin{0.4em}
\SetAlFnt{\small}
\begin{algorithm}[!t]
%\DontPrintSemicolon

\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetKwProg{Fn}{Function}{}{end}
\SetKwFunction{CST}{ComputeSuspendTasks}
\SetKwFunction{CS}{ComputeSpill}
\SetKwData{F}{\small{final}}

\Input{Array of running tasks $R$\;}
\Output{Array of suspended tasks $S$\;}
get the Memory Usage Sampler $Sampler$\;
get the Memory Manger $SM$ of System\;
get the Memory Manger $JM$ of JVM\;
get the queue including suspended tasks $SQ$\;
\lIf{Usage of $JM$ is lower than the yellow value}{return}
\lElseIf{Usage of $JM$ is lower than the red value}{\CS}
\lElseIf{$SQ$ is not empty}{return}
\lElse{\CST}

\BlankLine
\Fn{\CST}{
  $freeMemory \leftarrow JM.freeMemory$\;
  $consumption[] \leftarrow SM.tasksMemoryConsumption$\;
  $rate[] \leftarrow Sampler.getMemoryUsageRate$\;
  $percent[] \leftarrow Sampler.getCompletePercent$\;
  $S \leftarrow R$\;
  \While{$freeMemory > 0$}{
    $minRateTaskId \leftarrow reate[].min$\;   
    \lIf{\CS}{reduce the running cores and return}
    $memoryNecessary \leftarrow comsumption[taskId] * (1-percent[taskId])$\;
    $freeMemory -= memoryNecessary$ \;
    $S$ remove minRateTaskId \;
    push minRateTaskId to $SQ$\;
    $rate[]$ remove minRateTaskId \;
  }
  \KwRet{$S$}
}
\BlankLine
\Fn{\CS}{
  $taskId \leftarrow$ task Id \;
  $totalMemory \leftarrow JM.totalMemory$ \;
  \lIf{$comsumption[taskId] > totalMemory / R.length$}{\KwRet{True}}
  \Else{
      $memoryNecessary \leftarrow comsumption[taskId] / percent[taskId]$\;
      \lIf{$memoryNecessary > totalMemory / R.length$}{\KwRet{True}}
      \lElse{\KwRet{False}}
      }
}
\caption{The scheduling mechanism on JVM}
%\vspace{-4mm}  
\label{code:scheduler}
\end{algorithm}

The indicators of memory pressure serve as the basis of the memory usage rate based scheduler. The scheduling strategy is presented in Algorithm~\ref{code:scheduler}. The input of the scheduler is the current running tasks and the output is the proposed suspended tasks. The scheduler records the indicators of memory pressure. A \textit{Sampler} is designed to record the real-time values of the metrics regarding the currently running tasks. The Sampler runs seasonally to update the values of the metrics for each task and compute the current memory usage rate, such as the size of input dataset, the number of processed records, and the size of the results.

If the memory pressure is light or there are already suspended tasks, we return immediately without proposing suspended tasks (lines 4, 6). If the memory pressure has reached the red value, MURS will try to avoid spilling (line 5). When the memory usage of the JVM heap reaches the yellow value, we obtain the memory usage rates of all running tasks from the sampler and other details of current memory usage (lines 9-12). We use the measures to process the currently running tasks in the following order of memory usage model: constant, sub-linear, linear, and super-linear. The available memory is then the free memory subtracting the memory required for the currently running tasks. When the free memory is not enough for running the remaining unprocessed tasks, the scheduler will stop processing tasks (lines 14-20). This method is able to distinguish the tasks that have the sub-linear model but are run as the heavy tasks when processing the larger datasets, or the tasks that have the super-linear model but are run as the light tasks when they consume less memory. The processed tasks are removed and the remaining tasks will be returned. The returned tasks are heavy tasks and will be suspended. In order to avoid potential starvation, the suspended tasks are pushed to a queue (line 21). The FIFO algorithm allows us to resume the first suspended task to avoid starvation. 

MURS suspends the proposed heavy tasks to prevent memory pressure from increasing at a high rate. Note that if all running tasks are heavy tasks, MURS still schedules the tasks with the same algorithm. It always selects the tasks that have relatively lower memory usage rate out of all tasks. 

Function \textit{ComputeSpill} is designed to avoid spilling. As the running tasks share the JVM heap, the maximum memory space allowed for each task must be less than $M/N$ ($M$ is the memory size and $N$ is the number of running threads). When the actual memory consumption of a task exceeds the maximum space, the spill or out-of-memory error will occur. The running tasks that satisfy the condition of \textit{ComputeSpill} (lines 27, 30) are also suspended to reduce the degree of parallelism in order to acquire adequate memory space after the memory pressure is released.   

When a running task is completed, we resume the execution of a suspended task popped out of the queue. After the memory pressure recedes, reflected by the phenomenon that the usage of JVM heap decreases to below the yellow value after a full GC, the remaining suspended tasks will also be resumed. Namely, the completion of a running task only causes one suspended task to be resumed while the memory usage below yellow value will cause all suspended tasks to be resumed.

\begin{comment}
\subsection{Multi-launch with MURS}

When light tasks have a large proportion in the running tasks, or memory space is enough for total running tasks, the memory pressure is light. Light memory pressure usually means that memory space will suffer less allocation and reclamation. However, properly increase the frequency of memory allocation and reclamation can improve the efficiency of memory usage. With hyper-threading technology, we can increase the parallelism of submitted jobs as well as memory pressure. Fortunately, MURS releases our scruple that memory pressure increases seriously with high parallelism. Thus, we propose multi-launch to improve the efficiency of memory usage with MURS.

Multi-launch works when tasks are launched to workers. Multiple tasks are launched to workers compared to the original configuration. If the configuration has remained free CPUs in the workers, multi-launch will work better because an extra task can execute more quickly on a physical CPU than on a logic CPU. The multiple should not be too large, because 1) maximally accessible memory space in heap is limited; 2) the interprocessor communication time can not be ignored; 3) I/O parallelism is limited by disk.

\end{comment}








\begin{comment}
\subsection{Balance of the Stop}

When the heavy tasks are suspended, they will suffer from a long wait. A long wait will result in two problems: 1) CPU resource is waste during suspending tasks; and 2) stopped tasks can be stragglers. 

Although the tasks with linear memory usage model will lead to more memory pressure, frequent stopping is not tolerated in these tasks. And it is a little regretful to waste CPUs when some tasks are suspended. Multi-launch is designed to enhance the fairness:

\begin{enumerate}

\item When tasks are allocated to workers, multiple tasks will be launched. This will result in memory pressure at express speed.

\item After the mitigation of MURS, tasks with a constant or sub-linear memory usage rate will complete and memory pressure will recede. Thus, the tasks with a linear memory usage model will run smoothly with light memory pressure and more resources (both core and memory).

\end{enumerate}

However, multi-launch is not always advised. When the space of execution memory is much smaller or spill is accessed, multi-launch will affect the memory pressure to a great extent. Although our scheduler can prevent the increasing speed of memory pressure, much higher parallelism will decrease the available heap space of one task. Less max available space of one task will result in out of memory error.

As we know, stragglers are common in current distributed data processing systems. Many reasons, such as the heterogeneous environment~\cite{matei:heter}, can lead to one task taking much more time than other tasks and affects the completion of a job. Our scheduler even stops some tasks to prevent the execution. In fact, if the heterogeneous environment is gotten rid of, the cost of computation itself is much more stable. However, garbage collections can prominently increase the execution time of tasks, especially in current in-memory data processing systems, as shown in Figure~\ref{fig:memorypressure}. Our scheduler can control heavy memory pressure, based on the memory usage rate. With low memory pressure, these tasks can be free from the stop-the-world of garbage collection by waiting for constant or sub-linear tasks, and fortunately, the constant or sub-linear tasks can execute quickly with less memory pressure. Thus, we take some time to wait, but free more pressures of garbage collection. 
\end{comment}
