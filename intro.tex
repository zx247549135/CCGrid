\section{Introduction}

Many popular distributed data processing systems are sped up by in-memory computing model. Systems, such as Spark~\cite{zaharia2012resilient} and Flink~\cite{hueske2012opening}, are usually developed in Java, Scala, and other managed languages. In-memory data are stored as data objects in memory which have been proven to bloat the memory~\cite{bu:bloat}. Limited memory space for the submitted jobs will result in the memory pressure. Although these managed languages provide garbage collection(GC) to reclaim the useless data objects, caching data in memory may worsen the memory pressure, as they usually have a long lifetime in memory~\cite{lulu:deca}. 

Data processing systems usually work as batch processing systems. Jobs are submitted to systems individually and processed offline. Many enterprises also provide them as a server, such as Spark SQL~\cite{armbrust2015spark} and Hive~\cite{ashish:hive}, multiple jobs are submitted to the system and executed in the same context. Besides sharing the memory and cores, systems for service prefer to cache sharing data in memory in order to speed up all related jobs.
Some services are even oversold as it hypothesizes that all tenants will not submit their jobs at the same time. When these services are busy, multiple tasks will be launched to the cluster comparing with the original configurations. Data sharing and multi-launch all result in heavy memory pressure, and the memory pressure will affect all jobs, although some jobs only have light impact on memory pressure if they run alone in batch processing.

%Many popular distributed systems are speed up by in-memory data in current days. These systems, such as Spark~\cite{zaharia2012resilient} and Flink~\cite{hueske2012opening}, are usually implemented by Java, Scala and other object-oriented languages. Caching data in memory is the most familiar measure for these frameworks. These caching data are stored in the JVM heap or off heap~\cite{www:tungsten} along with the runtime data objects in JVM heap. Besides the direct memory allocation for frameworks themselves, some resource managers ~\cite{vavilapalli2013apache}, distributed cache and distributed file systems~\cite{www:alluxio} also require memory to provide service for these frameworks ~\cite{pu2016fairride, li2014tachyon}. Based on the tense memory allocation, the memory pressure in frameworks themselves is already overwhelmed~\cite{fang2015interruptible}, especially when it provides service for multi tenant.

There are two common ways, hot keys and large intermediate results, will lead to heavy memory pressure problem which harms the performance of data processing systems~\cite{fang2015interruptible}. Hot keys may result in an out-of-memory error, which may cause system crash. Large intermediate results, such as Java collection, may result in frequent garbage collection. The worst case occurs when the garbage collection time reaches a high percentage of execution time, performance of the data processing system will be extremely degraded, or system even crashes. In those systems for service, another particular problem is that some jobs have to suffer the inefficient memory pressure although they have light impact on memory. While most works mitigate memory pressure based on customized memory manager or application tuning in batch processing~\cite{www:spark-tuning, nguyen2015facade, fang2015interruptible, lulu:deca, nguyen:yak}, we intend to classify tasks those have greater impact on memory pressure in data processing systems for service. We briefly call task with greater impact on memory pressure as \textit{heavy task}, and task with light impact on memory pressure as \textit{light task}, respectively. Based on the classification, we suspend the heavy tasks to complete the light tasks, so that system resource is released and memory pressure is mitigated for all tasks.

%Heavy memory pressure will break the performance of current data processing systems obviously in two common ways: hot keys and large intermediate results~\cite{fang2015interruptible}. Hot keys can result in out of memory error which can crash the system. About 40\% memory problems are related to large intermediate results, such as large Java collections. Large intermediate results can result in frequent garbage collection. The worst case is while the frequent garbage collection reclaims less space it results in the vicious circle and quickly declines the performance or even crashes systems. Systems for service cannot tolerant the heavy memory pressure much more because the heavy memory pressure may be resulted in by only parts of the running tasks. While most works mitigate the memory pressure based on the memory manger or long lived data objects in batch processing~\cite{fang2015interruptible, nguyen2015facade}, we intend to classify these tasks who have more impact on memory pressure in data processing systems for service. Based on the classification, tasks with less impact on memory pressure can break away from heavy memory pressure and release resource quickly to mitigate the memory pressure for other tasks.

As memory pressure comes from massive long lived data objects produced by the running tasks in limited memory space, we find that the essential of long lived data objects is the function APIs called by tasks. These function APIs are based on key-value pairs~\cite{dean2008mapreduce, zaharia2012resilient, hueske2012opening, isard2007dryad} and the necessary memory space of different function APIs can be traced. The memory usage of these function APIs can be classified in four coarse-grained models: constant, sub-linear, linear, and super-linear. Each memory usage model has a different influence on memory. Task which is linear or super-linear model, or processing more input dataset, is closer to heavy task. These four models are combined independently in each task with strict order. We propose \textit{Memory Usage Rate} to determine to which model a task belongs. And we design a memory usage rate based scheduler, called MURS, which can efficiently mitigate heavy memory pressure and avoid memory overflow. The scheduler first collects the memory usage rate of current running tasks, and then selects the heavy tasks and suspend them. When heavy tasks are suspended, light tasks will complete quickly because memory pressure is light.

%As the memory pressure is coming from the massive long lived data object produced by the running tasks, tasks from different tenant will have different impact on memory pressure. Current data processing systems provide their own function APIs for user programs, and each task executes several function APIs~\cite{dean2008mapreduce, zaharia2012resilient, hueske2012opening, isard2007dryad}. We find that these function APIs are based on K-V pairs and the necessary memory space of different function APIs can be traced. The memory usage of these function APIs can be traced by four coarse-grained models: constant, sub-linear, linear and super-linear. Each model has different influence on memory, which means it can produce different memory pressure. Constant and sub-linear models have lighter effect on the memory pressure than linear model or super-linear model. These models are grouped independently in each task. We propose \textit{Memory Usage Rate} to distinguish which model one task belongs to. And we design a memory usage rate based scheduler called MURS which is proved to can avoid heavy memory pressure and spill. The scheduler gets the memory usage rate of current running tasks firstly, and chooses these tasks with linear or super-linear model to stop. When tasks with heavy influence on memory pressure is stopped, other tasks can complete quickly because memory pressure is light.

%Most works propose methods to slow down the memory pressure in batch processing, such as region based memory management~\cite{nguyen2015facade, gog2015broom}. However, the problem in multi-tenant is not always suited for these measures. Tenants who need less execution memory should not suffer heavy memory pressure poduced by other tenants. On the contrary, tenants who produce heavy memory pressure cannot get the resources of others immediately because others are prevented to complete early. The root of memory pressure is the running tasks who produce massive data objects in memory. Considering all tasks with the same characters is not appropriate in multi-tenant.

%Further more, users in both batch processing and multi-tenant write their codes by specific function APIs provided by frameworks, and then submit their jobs to system. As submitted job is always split into a set of tasks, each task executes several function APIs. Tasks running in the same time execute the same function APIs in batch processing but different function APIs in multi-tenant. Tasks with different function APIs have diverse effect on memory pressure undisputed, we find that although these tasks implement with the same function APIs, they can also have diverse effect on memory pressure because the distribution of keys in dataset also effect the memory pressure in some way. Thus, a uniform criterion to mark the influence on memory pressure of different tasks is most helpful to mange the tasks for both batch processing and multi-tenant.

%// 重点改



%In this paper, we analyze the characters of common function APIs in current popular data processing systems firstly. We propose \textit{Memory Usage Rate} to measure the influence one task has on the memory pressure. Four coarse grained types are designed to stand for different characters: constant, sub-linear, linear and super-linear. Constant tasks and sub-linear tasks have lighter effect on the memory pressure than linear tasks or super-leaner tasks. Based on the memory usage rate of current running tasks, we design a scheduler called MURS to schedule the tasks and avoid heavy memory pressure. The scheduler gets the memory usage rate of current running tasks in time and proposes the super linear or linear tasks to stop when the memory pressure comes. These stopped tasks will prevent heavy memory pressure because the heavy memory pressure is resulted in by themselves. Other tasks can run more quickly to empty the memory space for the stopped tasks.

%Last but not the least, we provide multi-launch to efficiently use memory. When light tasks have a large proportion in the running tasks, memory pressure is not heavy. Multi-launch will launch more tasks than original configuration to increase the parallelism as well as the memory pressure. In MURS, high parallelism can speed up the execution without considering heavy memory pressure. The multi-launch can be shut down optionally when the CPU resource is limited.

%Last but not the least, stopping tasks of some jobs is not appropriate in some way. Thus we provide multi launch to balance the stop, which can 1) speed up the occur of memory pressure and the response time for all submitted jobs; 2) increase the memory usage when all tasks have light pressure on memory by increasing parallelism. High parallelism instead of more memory pressure can improve the performance in some way. 

We implement MURS in Spark and the evaluation shows that the execution time can be reduced by up to 65.8\% (2.9X). The garbage collection which directly measures the memory pressure can be also decreased by a maximum of 81\%. MURS also extends the scalability of system, as systems can provide server although the memory resource is shortage. In summary, three main contributions of this paper are:

\begin{itemize}

\item We carefully analyze the memory utilization of variety data processing systems, and find that the memory space of a task to store long lived data objects has strong relationship to the function APIs provided by these frameworks.

%\item We propose memory usage rate which is bound to each task as a unified criterion for distinguishing their memory usage models in data processing systems for service. The criterion can extend to most data processing systems.

\item Based on our observation, we build four memory usage models which is independent of the data processing systems (or their function APIs) to measured the impact of a task on memory pressure, and propose memory usage rate to identify which model a task belongs to. We design the memory usage rate based scheduler called MURS which can substantially slow down the memory pressure.

%\item We propose multi launch to speed up the advent of memory pressure and increase the parallelism. The balance of pressure and parallelism can also improve the performance in some sense.

\item We implement a prototype system based on Spark. The proposed method can be ported to other similar data processing systems. The experimental results have shown that our system can significantly improve system performance, reduce garbage collection and extend the scalability of server.

\end{itemize}

The rest of the paper is organized as follows: Section II introduces the motivation of our work; Section III describes the four types of memory usage rate as the basic of Section IV which represents the scheduler in tasks with different memory usage rate; Section V describes the implementation of our scheduler; Evaluation is designed to show the advantages of our scheduler in Section VI; In Section VII and Section VIII, we discuss some related works and present our conclusion.